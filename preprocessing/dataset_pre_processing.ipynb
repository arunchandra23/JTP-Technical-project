{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation for Image Recommendation System\n",
    "\n",
    "This Jupyter notebook is dedicated to downloading and preprocessing the dataset for an image recommendation system. The dataset is sourced from Kaggle and consists of fashion product images. The preprocessing steps include dividing the dataset into train and test splits and organizing the images into respective directories for further processing.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Import necessary libraries and set up the environment.\n",
    "2. Configure Kaggle API and download the dataset.\n",
    "3. Extract the images from the downloaded zip file.\n",
    "4. Load and preprocess the metadata, and stratify the dataset into train and test splits.\n",
    "5. Copy the images to their respective train and test directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the Kaggle API by setting up the `kaggle.json` file and authenticate the API. Download the 'fashion-product-images-dataset' from Kaggle to a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the directory where you want to store the kaggle.json file\n",
    "kaggle_config_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "os.chmod(kaggle_config_dir, 0o700)\n",
    "\n",
    "# Copy the kaggle.json file to the directory\n",
    "kaggle_json_path = os.path.join(kaggle_config_dir, \"kaggle.json\")\n",
    "with open(\"../backend/model/kaggle.json\", \"r\") as source, open(kaggle_json_path, \"w\") as target:\n",
    "    target.write(source.read())\n",
    "\n",
    "# Set the permissions for the kaggle.json file (Linux/macOS only)\n",
    "os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "# Initialize the Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "dataset = \"paramaggarwal/fashion-product-images-dataset\"\n",
    "download_path = os.path.join(\"../backend\", \"data/\")\n",
    "\n",
    "zip_path = os.path.join(download_path, \"fashion-product-images-dataset.zip\")\n",
    "\n",
    "if(not os.path.exists(zip_path)):\n",
    "    api.dataset_download_files(dataset, path=download_path, unzip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory for storing images and extract only the 'images/' folder from the downloaded zip file. Remove the zip file after extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting images from the zip file...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_dataset = os.path.join(download_path, \"images\")\n",
    "os.makedirs(_dataset, exist_ok=True)\n",
    "\n",
    "# Unzip only the images/ folder\n",
    "\n",
    "print(\"Extracting images from the zip file...\")\n",
    "with zipfile.ZipFile('/home/arunchandra/Downloads/archive.zip', 'r') as zip_ref:\n",
    "    # Extract only the images/ folder\n",
    "    members = [m for m in zip_ref.namelist() if m.startswith(\"fashion-dataset/images/\")]\n",
    "    zip_ref.extractall(path=_dataset, members=members)\n",
    "\n",
    "# Remove the zip file\n",
    "os.remove(zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the metadata CSV file and fill missing values with default categories. Combine relevant columns for stratification and filter out classes with at least a minimum number of instances. Split the dataset into train and test sets based on the stratification column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../backend/model/data.csv')\n",
    "df.fillna({'masterCategory': 'Apparel', 'subCategory': 'Topwear', 'season': 'Summer', 'usage': 'Casual'}, inplace=True)\n",
    "# Combine the columns for stratification\n",
    "df['stratify_col'] = df['masterCategory'] + \"_\" + df['subCategory'] + \"_\" + df['season'] + \"_\" + df['usage']\n",
    "# Set a threshold for minimum instances required\n",
    "min_instances = 2\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = df['stratify_col'].value_counts()\n",
    "\n",
    "# Filter classes with at least min_instances\n",
    "valid_classes = class_counts[class_counts >= min_instances].index\n",
    "\n",
    "# Keep only the rows with valid classes\n",
    "df = df[df['stratify_col'].isin(valid_classes)]\n",
    "# Split the DataFrame into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['stratify_col'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directories for train and test images. Define a function to copy images from the extracted dataset to these directories based on the train and test DataFrame splits. Execute the function to populate the train and test directories with the respective images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have been successfully copied to train and test directories.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path=os.path.join(\"../backend\", \"data/Train/train\")\n",
    "test_path=os.path.join(\"../backend\", \"data/Test/test\")\n",
    "image_dataset_path=os.path.join(\"../backend\", \"data/images/fashion-dataset/images\")\n",
    "# Create directories for train and test images if they don't exist\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "# Function to copy images to the respective directory\n",
    "def copy_images(df, folder_name):\n",
    "    for _, row in df.iterrows():\n",
    "        src_path = os.path.join(image_dataset_path, f\"{row['id']}.jpg\")\n",
    "        dest_path = os.path.join(folder_name, f\"{row['id']}.jpg\")\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Copy images to the train and test directories\n",
    "copy_images(train_df, train_path)\n",
    "copy_images(test_df, test_path)\n",
    "print(\"Images have been successfully copied to train and test directories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
